2022-07-07 19:51:19,224 - train args
2022-07-07 19:51:19,224 - key: igmc_game
2022-07-07 19:51:19,224 - dataset: game
2022-07-07 19:51:19,224 - dataset_filename: game
2022-07-07 19:51:19,224 - keywords: None
2022-07-07 19:51:19,225 - model_type: IGMC
2022-07-07 19:51:19,225 - hop: 1
2022-07-07 19:51:19,225 - in_nfeats: 32
2022-07-07 19:51:19,225 - out_nfeats: 32
2022-07-07 19:51:19,225 - in_efeats: 32
2022-07-07 19:51:19,225 - out_efeats: 32
2022-07-07 19:51:19,225 - num_heads: 4
2022-07-07 19:51:19,225 - node_features: None
2022-07-07 19:51:19,225 - parameters: None
2022-07-07 19:51:19,225 - mpnn_type: None
2022-07-07 19:51:19,225 - edge_dropout: 0.3
2022-07-07 19:51:19,225 - latent_dims: None
2022-07-07 19:51:19,225 - device: 0
2022-07-07 19:51:19,225 - log_dir: log
2022-07-07 19:51:19,225 - log_interval: 100
2022-07-07 19:51:19,225 - train_lrs: [0.001]
2022-07-07 19:51:19,225 - train_epochs: 20
2022-07-07 19:51:19,225 - batch_size: 128
2022-07-07 19:51:19,225 - weight_decay: 0.0
2022-07-07 19:51:19,225 - lr_decay_step: 5
2022-07-07 19:51:19,225 - lr_decay_factor: 0.9
2022-07-07 19:52:11,667 - train args
2022-07-07 19:52:11,667 - key: igmc_game
2022-07-07 19:52:11,667 - dataset: game
2022-07-07 19:52:11,667 - dataset_filename: game
2022-07-07 19:52:11,667 - keywords: None
2022-07-07 19:52:11,667 - model_type: IGMC
2022-07-07 19:52:11,667 - hop: 1
2022-07-07 19:52:11,667 - in_nfeats: 32
2022-07-07 19:52:11,667 - out_nfeats: 32
2022-07-07 19:52:11,667 - in_efeats: 32
2022-07-07 19:52:11,667 - out_efeats: 32
2022-07-07 19:52:11,667 - num_heads: 4
2022-07-07 19:52:11,667 - node_features: None
2022-07-07 19:52:11,667 - parameters: None
2022-07-07 19:52:11,667 - mpnn_type: None
2022-07-07 19:52:11,667 - edge_dropout: 0.3
2022-07-07 19:52:11,667 - latent_dims: None
2022-07-07 19:52:11,667 - device: 0
2022-07-07 19:52:11,667 - log_dir: log
2022-07-07 19:52:11,667 - log_interval: 100
2022-07-07 19:52:11,667 - train_lrs: [0.001]
2022-07-07 19:52:11,667 - train_epochs: 20
2022-07-07 19:52:11,667 - batch_size: 128
2022-07-07 19:52:11,667 - weight_decay: 0.0
2022-07-07 19:52:11,667 - lr_decay_step: 5
2022-07-07 19:52:11,667 - lr_decay_factor: 0.9
2022-07-07 19:53:22,364 - train args
2022-07-07 19:53:22,364 - key: igmc_game
2022-07-07 19:53:22,364 - dataset: game
2022-07-07 19:53:22,364 - dataset_filename: game
2022-07-07 19:53:22,364 - keywords: None
2022-07-07 19:53:22,364 - model_type: IGMC
2022-07-07 19:53:22,364 - hop: 1
2022-07-07 19:53:22,364 - in_nfeats: 32
2022-07-07 19:53:22,364 - out_nfeats: 32
2022-07-07 19:53:22,364 - in_efeats: 32
2022-07-07 19:53:22,365 - out_efeats: 32
2022-07-07 19:53:22,365 - num_heads: 4
2022-07-07 19:53:22,365 - node_features: None
2022-07-07 19:53:22,365 - parameters: None
2022-07-07 19:53:22,365 - mpnn_type: None
2022-07-07 19:53:22,365 - edge_dropout: 0.3
2022-07-07 19:53:22,365 - latent_dims: [32, 32, 32, 32]
2022-07-07 19:53:22,365 - device: 0
2022-07-07 19:53:22,365 - log_dir: log
2022-07-07 19:53:22,365 - log_interval: 100
2022-07-07 19:53:22,365 - train_lrs: [0.001]
2022-07-07 19:53:22,365 - train_epochs: 20
2022-07-07 19:53:22,365 - batch_size: 128
2022-07-07 19:53:22,365 - weight_decay: 0.0
2022-07-07 19:53:22,365 - lr_decay_step: 5
2022-07-07 19:53:22,365 - lr_decay_factor: 0.9
2022-07-07 19:53:26,413 - Loading network finished ...

2022-07-07 19:53:26,414 - Start training ... learning rate : 0.001
2022-07-07 19:54:17,156 - train args
2022-07-07 19:54:17,156 - key: igmc_game
2022-07-07 19:54:17,156 - dataset: game
2022-07-07 19:54:17,156 - dataset_filename: game
2022-07-07 19:54:17,156 - keywords: None
2022-07-07 19:54:17,156 - model_type: IGMC
2022-07-07 19:54:17,156 - hop: 1
2022-07-07 19:54:17,156 - in_nfeats: 32
2022-07-07 19:54:17,156 - out_nfeats: 32
2022-07-07 19:54:17,156 - in_efeats: 32
2022-07-07 19:54:17,156 - out_efeats: 32
2022-07-07 19:54:17,156 - num_heads: 4
2022-07-07 19:54:17,156 - node_features: None
2022-07-07 19:54:17,156 - parameters: None
2022-07-07 19:54:17,157 - mpnn_type: None
2022-07-07 19:54:17,157 - edge_dropout: 0.3
2022-07-07 19:54:17,157 - latent_dims: [32, 32, 32, 32]
2022-07-07 19:54:17,157 - device: 0
2022-07-07 19:54:17,157 - log_dir: log
2022-07-07 19:54:17,157 - log_interval: 100
2022-07-07 19:54:17,157 - train_lrs: [0.001]
2022-07-07 19:54:17,157 - train_epochs: 20
2022-07-07 19:54:17,157 - batch_size: 128
2022-07-07 19:54:17,157 - weight_decay: 0.0
2022-07-07 19:54:17,157 - lr_decay_step: 5
2022-07-07 19:54:17,157 - lr_decay_factor: 0.9
2022-07-07 19:54:19,687 - Loading network finished ...

2022-07-07 19:54:19,687 - Start training ... learning rate : 0.001
2022-07-07 19:54:33,781 - === Epoch 1, train loss 0.077868, val rmse 0.248912, test rmse 0.241128 ===
2022-07-07 19:54:33,781 - new best test rmse 0.241128 ===
2022-07-07 19:54:46,604 - === Epoch 2, train loss 0.067650, val rmse 0.250401, test rmse 0.240595 ===
2022-07-07 19:54:46,604 - new best test rmse 0.240595 ===
2022-07-07 19:54:59,476 - === Epoch 3, train loss 0.063750, val rmse 0.248475, test rmse 0.238047 ===
2022-07-07 19:54:59,477 - new best test rmse 0.238047 ===
2022-07-07 19:55:13,081 - === Epoch 4, train loss 0.061431, val rmse 0.248455, test rmse 0.238854 ===
2022-07-07 19:55:26,237 - === Epoch 5, train loss 0.059999, val rmse 0.251074, test rmse 0.239115 ===
2022-07-07 19:55:39,223 - === Epoch 6, train loss 0.058816, val rmse 0.248273, test rmse 0.236584 ===
2022-07-07 19:55:39,224 - new best test rmse 0.236584 ===
2022-07-07 19:55:52,505 - === Epoch 7, train loss 0.058085, val rmse 0.248588, test rmse 0.237149 ===
2022-07-07 19:56:05,495 - === Epoch 8, train loss 0.058058, val rmse 0.251851, test rmse 0.239415 ===
2022-07-07 19:56:18,402 - === Epoch 9, train loss 0.057484, val rmse 0.248498, test rmse 0.236426 ===
2022-07-07 19:56:18,403 - new best test rmse 0.236426 ===
2022-07-07 19:56:31,469 - === Epoch 10, train loss 0.057207, val rmse 0.246208, test rmse 0.234232 ===
2022-07-07 19:56:31,470 - new best test rmse 0.234232 ===
2022-07-07 19:56:44,909 - === Epoch 11, train loss 0.056906, val rmse 0.248091, test rmse 0.234658 ===
2022-07-07 19:56:58,295 - === Epoch 12, train loss 0.057080, val rmse 0.247137, test rmse 0.234595 ===
2022-07-07 19:57:11,409 - === Epoch 13, train loss 0.056941, val rmse 0.246670, test rmse 0.234232 ===
2022-07-07 19:57:24,630 - === Epoch 14, train loss 0.056967, val rmse 0.247892, test rmse 0.235030 ===
2022-07-07 20:12:32,843 - train args
2022-07-07 20:12:32,843 - key: igmc_game
2022-07-07 20:12:32,843 - dataset: game
2022-07-07 20:12:32,843 - dataset_filename: game
2022-07-07 20:12:32,843 - keywords: None
2022-07-07 20:12:32,843 - model_type: IGMC
2022-07-07 20:12:32,843 - hop: 1
2022-07-07 20:12:32,843 - in_nfeats: 32
2022-07-07 20:12:32,843 - out_nfeats: 32
2022-07-07 20:12:32,843 - in_efeats: 32
2022-07-07 20:12:32,843 - out_efeats: 32
2022-07-07 20:12:32,843 - num_heads: 4
2022-07-07 20:12:32,843 - node_features: None
2022-07-07 20:12:32,843 - parameters: None
2022-07-07 20:12:32,843 - num_relations: 5
2022-07-07 20:12:32,843 - edge_dropout: 0.3
2022-07-07 20:12:32,843 - latent_dims: [32, 32, 32, 32]
2022-07-07 20:12:32,843 - device: 0
2022-07-07 20:12:32,843 - log_dir: log
2022-07-07 20:12:32,843 - log_interval: 100
2022-07-07 20:12:32,843 - train_lrs: [0.001, 0.002, 0.0005]
2022-07-07 20:12:32,843 - train_epochs: 20
2022-07-07 20:12:32,843 - batch_size: 128
2022-07-07 20:12:32,843 - weight_decay: 0.0
2022-07-07 20:12:32,843 - lr_decay_step: 5
2022-07-07 20:12:32,843 - lr_decay_factor: 0.9
2022-07-07 20:12:35,367 - Loading network finished ...

2022-07-07 20:12:35,367 - Start training ... learning rate : 0.001
2022-07-07 20:13:22,515 - train args
2022-07-07 20:13:22,515 - key: igmc_game
2022-07-07 20:13:22,515 - dataset: game
2022-07-07 20:13:22,515 - dataset_filename: game
2022-07-07 20:13:22,515 - keywords: None
2022-07-07 20:13:22,515 - model_type: IGMC
2022-07-07 20:13:22,515 - hop: 1
2022-07-07 20:13:22,515 - in_nfeats: 32
2022-07-07 20:13:22,515 - out_nfeats: 32
2022-07-07 20:13:22,515 - in_efeats: 32
2022-07-07 20:13:22,515 - out_efeats: 32
2022-07-07 20:13:22,515 - num_heads: 4
2022-07-07 20:13:22,515 - node_features: None
2022-07-07 20:13:22,515 - parameters: None
2022-07-07 20:13:22,515 - num_relations: 5
2022-07-07 20:13:22,515 - edge_dropout: 0.3
2022-07-07 20:13:22,515 - latent_dims: [32, 32, 32, 32]
2022-07-07 20:13:22,515 - device: 0
2022-07-07 20:13:22,515 - log_dir: log
2022-07-07 20:13:22,515 - log_interval: 100
2022-07-07 20:13:22,515 - train_lrs: [0.001, 0.002, 0.0005]
2022-07-07 20:13:22,515 - train_epochs: 20
2022-07-07 20:13:22,515 - batch_size: 128
2022-07-07 20:13:22,515 - weight_decay: 0.0
2022-07-07 20:13:22,515 - lr_decay_step: 5
2022-07-07 20:13:22,516 - lr_decay_factor: 0.9
2022-07-07 20:13:25,014 - Loading network finished ...

2022-07-07 20:13:25,014 - Start training ... learning rate : 0.001
2022-07-07 20:13:39,015 - === Epoch 1, train loss 0.080546, val rmse 0.253593, test rmse 0.245683 ===
2022-07-07 20:13:39,016 - new best test rmse 0.245683 ===
2022-07-07 20:13:53,505 - === Epoch 2, train loss 0.068125, val rmse 0.247773, test rmse 0.237257 ===
2022-07-07 20:13:53,505 - new best test rmse 0.237257 ===
2022-07-07 20:14:06,679 - === Epoch 3, train loss 0.065283, val rmse 0.251669, test rmse 0.241451 ===
2022-07-07 20:14:19,760 - === Epoch 4, train loss 0.062389, val rmse 0.247705, test rmse 0.238018 ===
2022-07-07 20:14:32,960 - === Epoch 5, train loss 0.061216, val rmse 0.251674, test rmse 0.240458 ===
2022-07-07 20:14:46,089 - === Epoch 6, train loss 0.059181, val rmse 0.247352, test rmse 0.235996 ===
2022-07-07 20:14:46,089 - new best test rmse 0.235996 ===
2022-07-07 20:14:59,225 - === Epoch 7, train loss 0.058648, val rmse 0.247543, test rmse 0.236038 ===
2022-07-07 20:15:12,417 - === Epoch 8, train loss 0.057888, val rmse 0.247667, test rmse 0.237273 ===
2022-07-07 20:15:25,627 - === Epoch 9, train loss 0.057990, val rmse 0.248928, test rmse 0.237309 ===
2022-07-07 20:15:38,844 - === Epoch 10, train loss 0.057587, val rmse 0.248175, test rmse 0.235896 ===
2022-07-07 20:15:38,845 - new best test rmse 0.235896 ===
2022-07-07 20:15:52,083 - === Epoch 11, train loss 0.056968, val rmse 0.249299, test rmse 0.236704 ===
2022-07-07 20:16:05,304 - === Epoch 12, train loss 0.056991, val rmse 0.248781, test rmse 0.235866 ===
2022-07-07 20:16:05,304 - new best test rmse 0.235866 ===
2022-07-07 20:16:18,726 - === Epoch 13, train loss 0.056952, val rmse 0.248623, test rmse 0.236406 ===
2022-07-07 20:16:32,521 - === Epoch 14, train loss 0.056942, val rmse 0.247617, test rmse 0.234486 ===
2022-07-07 20:16:32,522 - new best test rmse 0.234486 ===
2022-07-07 20:16:46,194 - === Epoch 15, train loss 0.056569, val rmse 0.247669, test rmse 0.234919 ===
2022-07-07 20:16:59,619 - === Epoch 16, train loss 0.056651, val rmse 0.247930, test rmse 0.235648 ===
2022-07-07 20:17:13,034 - === Epoch 17, train loss 0.056505, val rmse 0.246740, test rmse 0.234921 ===
2022-07-07 20:17:26,346 - === Epoch 18, train loss 0.056803, val rmse 0.247828, test rmse 0.235278 ===
2022-07-07 20:17:39,682 - === Epoch 19, train loss 0.056545, val rmse 0.246522, test rmse 0.234506 ===
2022-07-07 20:17:52,994 - === Epoch 20, train loss 0.056729, val rmse 0.246978, test rmse 0.236006 ===
2022-07-07 20:17:52,995 - Training ends. The best testing rmse is 0.234486 at epoch 14
2022-07-07 20:17:53,299 - Loading network finished ...

2022-07-07 20:17:53,299 - Start training ... learning rate : 0.002
2022-07-07 20:18:06,623 - === Epoch 1, train loss 0.082771, val rmse 0.248262, test rmse 0.236867 ===
2022-07-07 20:18:06,623 - new best test rmse 0.236867 ===
2022-07-07 20:18:20,032 - === Epoch 2, train loss 0.067743, val rmse 0.250246, test rmse 0.237910 ===
2022-07-07 20:18:33,579 - === Epoch 3, train loss 0.062449, val rmse 0.248875, test rmse 0.237821 ===
2022-07-07 20:18:47,027 - === Epoch 4, train loss 0.059923, val rmse 0.253043, test rmse 0.242799 ===
2022-07-07 20:19:00,589 - === Epoch 5, train loss 0.059309, val rmse 0.247598, test rmse 0.235514 ===
2022-07-07 20:19:00,589 - new best test rmse 0.235514 ===
2022-07-07 20:19:14,351 - === Epoch 6, train loss 0.058039, val rmse 0.247398, test rmse 0.236375 ===
2022-07-07 20:19:28,107 - === Epoch 7, train loss 0.057741, val rmse 0.246547, test rmse 0.234685 ===
2022-07-07 20:19:28,107 - new best test rmse 0.234685 ===
2022-07-07 20:19:42,090 - === Epoch 8, train loss 0.057615, val rmse 0.248199, test rmse 0.236704 ===
2022-07-07 20:19:55,694 - === Epoch 9, train loss 0.057487, val rmse 0.248093, test rmse 0.235834 ===
2022-07-07 20:20:09,162 - === Epoch 10, train loss 0.058332, val rmse 0.247281, test rmse 0.235299 ===
2022-07-07 20:20:22,659 - === Epoch 11, train loss 0.057572, val rmse 0.248237, test rmse 0.234955 ===
2022-07-07 20:20:36,196 - === Epoch 12, train loss 0.057691, val rmse 0.247105, test rmse 0.235416 ===
2022-07-07 20:20:49,688 - === Epoch 13, train loss 0.057797, val rmse 0.247146, test rmse 0.234877 ===
2022-07-07 20:21:03,172 - === Epoch 14, train loss 0.057926, val rmse 0.250670, test rmse 0.238087 ===
2022-07-07 20:21:16,708 - === Epoch 15, train loss 0.057702, val rmse 0.247026, test rmse 0.235788 ===
2022-07-07 20:21:30,235 - === Epoch 16, train loss 0.057798, val rmse 0.246866, test rmse 0.234845 ===
2022-07-07 20:21:43,724 - === Epoch 17, train loss 0.057408, val rmse 0.249224, test rmse 0.237394 ===
2022-07-07 20:21:57,281 - === Epoch 18, train loss 0.057522, val rmse 0.247450, test rmse 0.236422 ===
2022-07-07 20:22:10,835 - === Epoch 19, train loss 0.057303, val rmse 0.246665, test rmse 0.235020 ===
2022-07-07 20:22:24,397 - === Epoch 20, train loss 0.057460, val rmse 0.248479, test rmse 0.236252 ===
2022-07-07 20:22:24,399 - Training ends. The best testing rmse is 0.234685 at epoch 7
2022-07-07 20:22:24,698 - Loading network finished ...

2022-07-07 20:22:24,698 - Start training ... learning rate : 0.0005
2022-07-07 20:22:38,647 - === Epoch 1, train loss 0.088234, val rmse 0.251314, test rmse 0.245759 ===
2022-07-07 20:22:38,647 - new best test rmse 0.245759 ===
2022-07-07 20:22:52,290 - === Epoch 2, train loss 0.073618, val rmse 0.251548, test rmse 0.244522 ===
2022-07-07 20:22:52,290 - new best test rmse 0.244522 ===
2022-07-07 20:23:05,700 - === Epoch 3, train loss 0.069890, val rmse 0.248276, test rmse 0.239030 ===
2022-07-07 20:23:05,700 - new best test rmse 0.239030 ===
2022-07-07 20:23:19,181 - === Epoch 4, train loss 0.067429, val rmse 0.252019, test rmse 0.247755 ===
2022-07-07 20:23:32,699 - === Epoch 5, train loss 0.065788, val rmse 0.247268, test rmse 0.240699 ===
2022-07-07 20:23:46,191 - === Epoch 6, train loss 0.064456, val rmse 0.247187, test rmse 0.241236 ===
2022-07-07 20:23:59,697 - === Epoch 7, train loss 0.062770, val rmse 0.247388, test rmse 0.239749 ===
2022-07-07 20:24:13,200 - === Epoch 8, train loss 0.062103, val rmse 0.249520, test rmse 0.240978 ===
2022-07-07 20:24:26,721 - === Epoch 9, train loss 0.061208, val rmse 0.247277, test rmse 0.239124 ===
2022-07-07 20:24:40,187 - === Epoch 10, train loss 0.060526, val rmse 0.248147, test rmse 0.240090 ===
2022-07-07 20:24:53,841 - === Epoch 11, train loss 0.059881, val rmse 0.257694, test rmse 0.251214 ===
2022-07-07 20:25:07,519 - === Epoch 12, train loss 0.059400, val rmse 0.246553, test rmse 0.237340 ===
2022-07-07 20:25:07,520 - new best test rmse 0.237340 ===
2022-07-07 20:25:21,341 - === Epoch 13, train loss 0.058888, val rmse 0.249115, test rmse 0.237767 ===
2022-07-07 20:25:35,306 - === Epoch 14, train loss 0.058565, val rmse 0.247871, test rmse 0.236952 ===
2022-07-07 20:25:35,306 - new best test rmse 0.236952 ===
2022-07-07 20:25:49,222 - === Epoch 15, train loss 0.058197, val rmse 0.248062, test rmse 0.237507 ===
2022-07-07 20:26:02,992 - === Epoch 16, train loss 0.057777, val rmse 0.246719, test rmse 0.237220 ===
2022-07-07 20:26:16,509 - === Epoch 17, train loss 0.057281, val rmse 0.246638, test rmse 0.236672 ===
2022-07-07 20:26:16,510 - new best test rmse 0.236672 ===
2022-07-07 20:26:30,034 - === Epoch 18, train loss 0.057285, val rmse 0.246885, test rmse 0.235865 ===
2022-07-07 20:26:30,034 - new best test rmse 0.235865 ===
2022-07-07 20:26:43,494 - === Epoch 19, train loss 0.057338, val rmse 0.246856, test rmse 0.235120 ===
2022-07-07 20:26:43,494 - new best test rmse 0.235120 ===
2022-07-07 20:26:56,994 - === Epoch 20, train loss 0.056602, val rmse 0.246871, test rmse 0.235901 ===
2022-07-07 20:26:56,995 - Training ends. The best testing rmse is 0.235120 at epoch 19
2022-07-07 20:26:57,021 - **********The final best testing RMSE is 0.234486 at lr 0.001********
